{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03aafdc0",
   "metadata": {},
   "source": [
    "#   Tarea 2 pregunta 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5badb",
   "metadata": {},
   "source": [
    "##  Importacion de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45e0c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras    \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156da4ca",
   "metadata": {},
   "source": [
    "##  Extraccion de datos y tratamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9686ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_path=\"spa.txt\"\n",
    "with open(text_file_path) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    spa = \"[start] \" + spa + \" [end]\"\n",
    "    text_pairs.append((eng, spa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e675a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93cfd8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar texto\n",
    "strip_chars = string.punctuation + \"Â¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size      = 15_000\n",
    "sequence_length = 20\n",
    "batch_size      = 64\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf_strings.lower(input_string)\n",
    "    return tf_strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "# Vectorizar\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens  = vocab_size,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "spa_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_spa_texts = [pair[1] for pair in train_pairs]\n",
    "eng_vectorization.adapt(train_eng_texts)\n",
    "spa_vectorization.adapt(train_spa_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "857a7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(eng, spa):\n",
    "    eng = eng_vectorization(eng)\n",
    "    spa = spa_vectorization(spa)\n",
    "    return (\n",
    "        {\n",
    "            \"english\": eng,\n",
    "            \"spanish\": spa[:, :-1],\n",
    "        },\n",
    "        spa[:, 1:],\n",
    "    )\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.cache().shuffle(2048).prefetch(16)\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds   = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "345838ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"english\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"spanish\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa195ad",
   "metadata": {},
   "source": [
    "##  Hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a041c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256 \n",
    "dense_dim = 2048 \n",
    "num_heads = 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cfd4f5",
   "metadata": {},
   "source": [
    "##  Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc113a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(  input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)  \n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions \n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):   \n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):   \n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,})  \n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8f9dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim          \n",
    "        self.dense_dim = dense_dim        \n",
    "        self.num_heads = num_heads        \n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation=\"relu\"),\n",
    "                                            layers.Dense(embed_dim),])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):   \n",
    "        if mask is not None:                 \n",
    "            mask = mask[:, tf.newaxis, :]    \n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input  = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self):   \n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "        \"embed_dim\": self.embed_dim,\n",
    "        \"num_heads\": self.num_heads,\n",
    "        \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24cadc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential( [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "                                             layers.Dense(embed_dim),])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "        \"embed_dim\": self.embed_dim,\n",
    "        \"num_heads\": self.num_heads,\n",
    "        \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")  \n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))    \n",
    "        mult = tf.concat(         \n",
    "            [tf.expand_dims(batch_size, -1),              \n",
    "            tf.constant([1, 1], dtype=tf.int32)], axis=0)         \n",
    "        return tf.tile(mask, mult)\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs) \n",
    "        if mask is not None:   \n",
    "            padding_mask = tf.cast(   \n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")   \n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)  \n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask) \n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,   \n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b0b3e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x) \n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs) \n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)  \n",
    "\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ba18fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d88553a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " english (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " spanish (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, None, 256)   3845120     ['english[0][0]']                \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " positional_embedding_1 (Positi  (None, None, 256)   3845120     ['spanish[0][0]']                \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, None, 256)   5259520     ['positional_embedding_1[0][0]', \n",
      " erDecoder)                                                       'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 256)    0           ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, None, 15000)  3855000     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19,960,216\n",
      "Trainable params: 19,960,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5adb1b",
   "metadata": {},
   "source": [
    "##  Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10f5f886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1302/1302 [==============================] - 159s 116ms/step - loss: 1.6823 - accuracy: 0.4142 - val_loss: 1.3289 - val_accuracy: 0.5040\n",
      "Epoch 2/10\n",
      "1302/1302 [==============================] - 149s 114ms/step - loss: 1.3495 - accuracy: 0.5285 - val_loss: 1.1970 - val_accuracy: 0.5523\n",
      "Epoch 3/10\n",
      "1302/1302 [==============================] - 150s 115ms/step - loss: 1.1940 - accuracy: 0.5786 - val_loss: 1.0899 - val_accuracy: 0.5982\n",
      "Epoch 4/10\n",
      "1302/1302 [==============================] - 174s 133ms/step - loss: 1.1028 - accuracy: 0.6116 - val_loss: 1.0440 - val_accuracy: 0.6194\n",
      "Epoch 5/10\n",
      "1302/1302 [==============================] - 250s 192ms/step - loss: 1.0544 - accuracy: 0.6335 - val_loss: 1.0164 - val_accuracy: 0.6353\n",
      "Epoch 6/10\n",
      "1302/1302 [==============================] - 251s 193ms/step - loss: 1.0237 - accuracy: 0.6484 - val_loss: 1.0121 - val_accuracy: 0.6382\n",
      "Epoch 7/10\n",
      "1302/1302 [==============================] - 250s 192ms/step - loss: 1.0031 - accuracy: 0.6602 - val_loss: 0.9996 - val_accuracy: 0.6486\n",
      "Epoch 8/10\n",
      "1302/1302 [==============================] - 249s 192ms/step - loss: 0.9850 - accuracy: 0.6700 - val_loss: 0.9924 - val_accuracy: 0.6518\n",
      "Epoch 9/10\n",
      "1302/1302 [==============================] - 250s 192ms/step - loss: 0.9701 - accuracy: 0.6790 - val_loss: 0.9962 - val_accuracy: 0.6469\n",
      "Epoch 10/10\n",
      "1302/1302 [==============================] - 250s 192ms/step - loss: 0.9552 - accuracy: 0.6862 - val_loss: 0.9801 - val_accuracy: 0.6582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1efaebb0cd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCAS = 10\n",
    "transformer.fit(train_ds, epochs=EPOCAS, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3fdf48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_vocab = spa_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    decoded_sentence         = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer(\n",
    "            {\n",
    "                \"english\": tokenized_input_sentence,\n",
    "                \"spanish\": tokenized_target_sentence,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        #sampled_token_index = tf.convert_to_numpy(tf.argmax(predictions[0, i, :])).item(0)\n",
    "        sampled_token_index = tf.argmax(predictions[0, i, :]).numpy().item(0)\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "081e639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "# almacenar un diccionario de predicciones\n",
    "predictions_dict = {}\n",
    "for _ in range(30):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated     = decode_sequence(input_sentence)\n",
    "    predictions_dict[input_sentence] = translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b184d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Aren't you in town?\": '[start] no estÃ¡s en la ciudad [end]',\n",
       " 'I am going to stay here till the day after tomorrow.': '[start] maÃ±ana me voy a estar aquÃ­ hasta el dÃ­a [end]',\n",
       " 'Where did you see the boy?': '[start] dÃ³nde viste el niÃ±o [end]',\n",
       " \"I'm beginning to lose my patience.\": '[start] estoy pasando que me [UNK] la mala [end]',\n",
       " 'I need one more favor.': '[start] necesito un favor mÃ¡s [end]',\n",
       " 'You have changed so much that I can hardly recognize you.': '[start] has tenido tanto que puedo a las [UNK] que te lo [UNK] [end]',\n",
       " 'Send Tom a message.': '[start] [UNK] a tom un mensaje [end]',\n",
       " \"That's all we know.\": '[start] es todo lo que nosotros [end]',\n",
       " \"Look! There's a cat in the kitchen.\": '[start] mira hay un gato en la cocina [end]',\n",
       " 'I have a terrible pain.': '[start] tengo un dolor de cabeza [end]',\n",
       " 'Tom grew up with Mary and John.': '[start] tom se fue a mary con john [end]',\n",
       " 'Forgiveness is the greatest gift of all.': '[start] [UNK] es el mejor libro de verdad [end]',\n",
       " 'I need some time to think.': '[start] necesito algo de tiempo para pensar [end]',\n",
       " 'I was scratched by a cat.': '[start] me [UNK] un gato [end]',\n",
       " \"Tom wouldn't approve.\": '[start] tom no fue [UNK] [end]',\n",
       " 'I wonder why he is late.': '[start] me pregunto por quÃ© Ã©l llegÃ³ tarde [end]',\n",
       " \"You're the best.\": '[start] eres el mejor [end]',\n",
       " \"I think it's too cold to swim.\": '[start] pienso que es demasiado frÃ­o para nadar [end]',\n",
       " 'They all knew.': '[start] todos se estaban [end]',\n",
       " 'I am the tallest in our class.': '[start] soy el mÃ¡s alto de nuestro clase [end]',\n",
       " 'When he was a child, his ambition was to be an English teacher.': '[start] cuando era un niÃ±o Ã©l su [UNK] era estar con profesor de inglÃ©s [end]',\n",
       " \"Isn't that awesome?\": '[start] no es tan [UNK] [end]',\n",
       " 'I did my job.': '[start] hice mi trabajo [end]',\n",
       " \"Do you think I'm sexy?\": '[start] crees que soy una [UNK] [end]',\n",
       " \"Tom didn't dare to look at Mary.\": '[start] tom no se [UNK] a mary [end]',\n",
       " \"Tom asked me why I couldn't go.\": '[start] tom me pidiÃ³ que no pude ir [end]',\n",
       " \"I waited for him at the station for an hour, but he didn't show up.\": '[start] lo he terminado por la estaciÃ³n por una hora pero no se lo [UNK] [end]',\n",
       " 'We played cards to kill time.': '[start] nos [UNK] a jugar tiempo [end]',\n",
       " 'Could you come to my quarters?': '[start] puedes venir a mi [UNK] de [UNK] [end]',\n",
       " \"It's to the point.\": '[start] es difÃ­cil [end]'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5bbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IMA543_2025_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
